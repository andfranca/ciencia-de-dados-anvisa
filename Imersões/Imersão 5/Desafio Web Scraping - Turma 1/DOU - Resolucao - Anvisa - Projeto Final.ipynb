{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79787220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d27aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRUPO 1\n",
    "def listar_publicacoes(data):\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    url = 'https://www.in.gov.br/leiturajornal'\n",
    "\n",
    "    data = data\n",
    "    orgao = 'Ministério-da-Saúde'\n",
    "    orgao_sub = 'Agência-Nacional-de-Vigilância-Sanitária'\n",
    "    ato = 'Resolução'\n",
    "\n",
    "    url_base = f'https://www.in.gov.br/leiturajornal?secao=dou1&data={data}&org={orgao}&org_sub={orgao_sub}&ato={ato}'\n",
    "\n",
    "    driver.get(url_base)\n",
    "               \n",
    "    html_content = driver.page_source\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    padrao = re.compile(r'/web/dou/-/resolucao.*')\n",
    "    links = soup.find_all('a', href=padrao)\n",
    "\n",
    "    links_filtrados = [] \n",
    "    for link in links:\n",
    "        li = link.get('href')\n",
    "        links_filtrados.append(f'https://www.in.gov.br/{li}')\n",
    "       \n",
    "        \n",
    "    return(links_filtrados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_informacoes(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # GRUPO 2\n",
    "    publicado_em_elemento = soup.find('span', {'class': 'publicado-dou-data'})\n",
    "    publicado_em = publicado_em_elemento.text.strip() if publicado_em_elemento is not None else None\n",
    "    \n",
    "    edicao_elemento = soup.find('span', {'class': 'edicao-dou-data'})\n",
    "    edicao = edicao_elemento.text.strip() if edicao_elemento is not None else None\n",
    "    \n",
    "    secao_elemento = soup.find('span', {'class': 'secao-dou'})\n",
    "    secao = secao_elemento.text.strip() if secao_elemento is not None else None\n",
    "    \n",
    "    pagina_elemento = soup.find('span', {'class': 'secao-dou-data'})\n",
    "    pagina = pagina_elemento.text.strip() if pagina_elemento is not None else None\n",
    "    \n",
    "    orgao_elemento = soup.find('span', {'class': 'orgao-dou-data'})\n",
    "    orgao = orgao_elemento.text.strip() if orgao_elemento is not None else None\n",
    "    \n",
    "    assina_elemento = soup.find('p', {'class': 'assina'})\n",
    "    assina = assina_elemento.text.strip() if assina_elemento is not None else None\n",
    "    \n",
    "    identifica_elemento = soup.find('p', {'class': 'identifica'})\n",
    "    identifica = identifica_elemento.text.strip() if identifica_elemento is not None else None\n",
    "    \n",
    "    preambulo_elemento = soup.find('p', {'class': 'dou-paragraph'})\n",
    "    preambulo = preambulo_elemento.text.strip() if preambulo_elemento is not None else None\n",
    "    \n",
    "    versao_oficial_elemento = soup.find('a', {'id': 'versao-certificada'})\n",
    "    versao = versao_oficial_elemento.get('href') if versao_oficial_elemento is not None else None\n",
    "    \n",
    "    artigos = re.compile(r'Art\\.\\s?\\d+º')\n",
    "    paragrafos_artigos = []\n",
    "\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        if artigos.search(paragraph.text):\n",
    "            paragrafos_artigos.append(paragraph.text.strip())\n",
    "\n",
    "    #GRUPO 3\n",
    "    #Extrair número do Expediente\n",
    "    expediente_elements = soup.find_all('p', text=re.compile(r'Expediente'))\n",
    "    numeros_expediente = []\n",
    "    for expediente_element in expediente_elements:\n",
    "        expediente_text = expediente_element.text.strip()\n",
    "        numero_expediente = re.search(r'(\\d+/\\d+-\\d)', expediente_text)\n",
    "        if numero_expediente:\n",
    "            numeros_expediente.append(numero_expediente.group(1))\n",
    "\n",
    "    #Extrair Texto completo\n",
    "    completo = []\n",
    "    for i in soup.find_all('p', {'class':'dou-paragraph'}):\n",
    "        completo.append(i.text.strip())\n",
    "       \n",
    "    #Ajuste fino\n",
    "       \n",
    "    data = {\n",
    "    'DT_PUBLICACAO': [publicado_em],\n",
    "    'EDICAO': [edicao],\n",
    "    'SECAO': [secao],\n",
    "    'PAGINA': [pagina],\n",
    "    'ORGAO':[orgao],\n",
    "    'ASSINATURA':[assina],\n",
    "    'PREAMBULO':[preambulo],\n",
    "    'ARTIGOS':[paragrafos_artigos],\n",
    "    'IDENTIFICACAO':[identifica], \n",
    "    'VERSAO_OFICIAL': [versao],\n",
    "    'EXPEDIENTE': [numeros_expediente],\n",
    "    'TEXTO_COMPLETO':[completo]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba1d4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
